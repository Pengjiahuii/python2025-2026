import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin


def fetch_xinhua_latest():
    url = "https://www.news.cn/"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/118.0 Safari/537.36"
        )
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        print("ğŸ“¢ æ–°åç½‘æ–°é—»æå–ç»“æœ")
        print("=" * 60)

        # å®šä¹‰å…³é”®è¯åŒ¹é…ï¼ˆå› ä¸ºæ–°ç‰ˆé¡µé¢ class åä¼šå˜åŒ–ï¼‰
        sections = {
            "ç„¦ç‚¹æ–°é—»": ["focus-newsMedia", "focus-news", "headlines", "photo-news"],
            "ç½®é¡¶æ–°é—»": ["focus-newsMedia-top", "top-news", "headline"],
            "æ–°åç³»åˆ—æŠ¥åˆŠ": ["channel newspapers", "newspaper", "bkdy"]
        }

        all_news = []

        for section_name, possible_classes in sections.items():
            print(f"\nğŸ¯ {section_name}:")
            found = False
            for cls in possible_classes:
                container = soup.find('div', class_=cls) or soup.find('section', class_=cls)
                if container:
                    found = True
                    links = container.find_all('a', href=True)
                    count = 0
                    for link in links:
                        title = link.get_text(strip=True)
                        href = link.get('href')
                        if title and len(title) > 4:
                            full_url = urljoin(url, href)
                            count += 1
                            print(f"{count}. {title}")
                            print(f"   é“¾æ¥: {full_url}")
                            all_news.append((title, full_url))
                    if count == 0:
                        print("   æš‚æ— å¯ç”¨æ–°é—»é“¾æ¥ã€‚")
                    break
            if not found:
                print("   æœªæ‰¾åˆ°è¯¥åˆ†ç±»çš„æ–°é—»åŒºå—ã€‚")

        print(f"\nâœ… å…±æå–åˆ° {len(all_news)} æ¡æ–°é—»ã€‚")
        return all_news

    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸ è§£æé”™è¯¯: {e}")
        return []


if __name__ == "__main__":
    news_list = fetch_xinhua_latest()
